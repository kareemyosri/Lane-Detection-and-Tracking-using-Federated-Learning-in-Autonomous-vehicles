{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd4c815",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc622d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "import shutil\n",
    "import socket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb75ce",
   "metadata": {},
   "source": [
    "# Dataset preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadbcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator2D(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_path, img_size=256, batch_size=1, shuffle=True):\n",
    "\n",
    "        self.base_path = base_path\n",
    "        self.img_size = img_size\n",
    "        self.id = os.listdir(os.path.join(base_path, \"gt_image\"))\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.id) / float(self.batch_size)))\n",
    "\n",
    "    def __load__(self, id_name):\n",
    "        image_path = os.path.join(self.base_path, \"gt_image\", (id_name ))\n",
    "        label_path = os.path.join(self.base_path, \"gt_binary_image\", (id_name ))\n",
    "        #print(image_path)\n",
    "        image = cv2.imread(image_path, 1)  # Reading Image in RGB format\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        # image = cv2.resize(image, (int(img.shape[1]/2), int(img.shape[0]/2)))\n",
    "        #print(label_path)\n",
    "        mask = cv2.imread(label_path, 1)\n",
    "        mask = cv2.resize(mask, (self.img_size, self.img_size))\n",
    "        # mask = cv2.resize(mask, (int(img.shape[1]/2), int(img.shape[0]/2)))\n",
    "\n",
    "        # Normalizing the image\n",
    "        image = image / 255.0\n",
    "        mask = mask / 255.0\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if (index + 1) * self.batch_size > len(self.id):\n",
    "            file_batch = self.id[index * self.batch_size:]\n",
    "        else:\n",
    "            file_batch = self.id[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        images, masks = [], []\n",
    "\n",
    "        for id_name in file_batch:\n",
    "            _img, _mask = self.__load__(id_name)\n",
    "            images.append(_img)\n",
    "            masks.append(_mask)\n",
    "\n",
    "        images = np.array(images)\n",
    "        masks = np.array(masks)\n",
    "        #masks = masks.reshape((masks.shape[0], self.img_size, self.img_size,1))\n",
    "        return images, masks\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.id))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.id))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd4099",
   "metadata": {},
   "source": [
    "# Model Blocks and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13f1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_excite_block(inputs, ratio=8):\n",
    "    init = inputs\n",
    "    channel_axis = -1\n",
    "    filters = init.shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    x = Multiply()([init, se])\n",
    "    return x\n",
    "\n",
    "def stem_block(x, n_filter, strides):\n",
    "    x_init = x\n",
    "\n",
    "    ## Conv 1\n",
    "    x = Conv2D(n_filter, (3, 3), padding=\"same\", strides=strides)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(n_filter, (3, 3), padding=\"same\")(x)\n",
    "\n",
    "    ## Shortcut\n",
    "    s  = Conv2D(n_filter, (1, 1), padding=\"same\", strides=strides)(x_init)\n",
    "    s = BatchNormalization()(s)\n",
    "\n",
    "    ## Add\n",
    "    x = Add()([x, s])\n",
    "    x = squeeze_excite_block(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_block(x, n_filter, strides=1):\n",
    "    x_init = x\n",
    "\n",
    "    ## Conv 1\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(n_filter, (3, 3), padding=\"same\", strides=strides)(x)\n",
    "    ## Conv 2\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(n_filter, (3, 3), padding=\"same\", strides=1)(x)\n",
    "\n",
    "    ## Shortcut\n",
    "    s  = Conv2D(n_filter, (1, 1), padding=\"same\", strides=strides)(x_init)\n",
    "    s = BatchNormalization()(s)\n",
    "\n",
    "    ## Add\n",
    "    x = Add()([x, s])\n",
    "    x = squeeze_excite_block(x)\n",
    "    return x\n",
    "\n",
    "def aspp_block(x, num_filters, rate_scale=1):\n",
    "    x1 = Conv2D(num_filters, (3, 3), dilation_rate=(6 * rate_scale, 6 * rate_scale), padding=\"same\")(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "\n",
    "    x2 = Conv2D(num_filters, (3, 3), dilation_rate=(12 * rate_scale, 12 * rate_scale), padding=\"same\")(x)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "\n",
    "    x3 = Conv2D(num_filters, (3, 3), dilation_rate=(18 * rate_scale, 18 * rate_scale), padding=\"same\")(x)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "\n",
    "    x4 = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "\n",
    "    y = Add()([x1, x2, x3, x4])\n",
    "    y = Conv2D(num_filters, (1, 1), padding=\"same\")(y)\n",
    "    return y\n",
    "\n",
    "def attetion_block(g, x):\n",
    "    \"\"\"\n",
    "        g: Output of Parallel Encoder block\n",
    "        x: Output of Previous Decoder block\n",
    "    \"\"\"\n",
    "\n",
    "    filters = x.shape[-1]\n",
    "\n",
    "    g_conv = BatchNormalization()(g)\n",
    "    g_conv = Activation(\"relu\")(g_conv)\n",
    "    g_conv = Conv2D(filters, (3, 3), padding=\"same\")(g_conv)\n",
    "\n",
    "    g_pool = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(g_conv)\n",
    "\n",
    "    x_conv = BatchNormalization()(x)\n",
    "    x_conv = Activation(\"relu\")(x_conv)\n",
    "    x_conv = Conv2D(filters, (3, 3), padding=\"same\")(x_conv)\n",
    "\n",
    "    gc_sum = Add()([g_pool, x_conv])\n",
    "\n",
    "    gc_conv = BatchNormalization()(gc_sum)\n",
    "    gc_conv = Activation(\"relu\")(gc_conv)\n",
    "    gc_conv = Conv2D(filters, (3, 3), padding=\"same\")(gc_conv)\n",
    "\n",
    "    gc_mul = Multiply()([gc_conv, x])\n",
    "    return gc_mul\n",
    "\n",
    "def Resunetpp(sz = (256, 256, 3)):\n",
    "    x = Input(sz)\n",
    "    n_filters = [16, 32, 64, 128, 256]\n",
    "    c0 = x\n",
    "    c1 = stem_block(c0, n_filters[0], strides=1)\n",
    "\n",
    "    ## Encoder\n",
    "    c2 = resnet_block(c1, n_filters[1], strides=2)\n",
    "    c3 = resnet_block(c2, n_filters[2], strides=2)\n",
    "    c4 = resnet_block(c3, n_filters[3], strides=2)\n",
    "\n",
    "    ## Bridge\n",
    "    b1 = aspp_block(c4, n_filters[4])\n",
    "\n",
    "    ## Decoder\n",
    "    d1 = attetion_block(c3, b1)\n",
    "    d1 = UpSampling2D((2, 2))(d1)\n",
    "    d1 = Concatenate()([d1, c3])\n",
    "    d1 = resnet_block(d1, n_filters[3])\n",
    "\n",
    "    d2 = attetion_block(c2, d1)\n",
    "    d2 = UpSampling2D((2, 2))(d2)\n",
    "    d2 = Concatenate()([d2, c2])\n",
    "    d2 = resnet_block(d2, n_filters[2])\n",
    "\n",
    "    d3 = attetion_block(c1, d2)\n",
    "    d3 = UpSampling2D((2, 2))(d3)\n",
    "    d3 = Concatenate()([d3, c1])\n",
    "    d3 = resnet_block(d3, n_filters[1])\n",
    "\n",
    "    ## output\n",
    "    outputs = aspp_block(d3, n_filters[0])\n",
    "    outputs = Conv2D(1, (1, 1), padding=\"same\")(outputs)\n",
    "    outputs = Activation(\"sigmoid\")(outputs)\n",
    "\n",
    "    ## Model\n",
    "    model = Model(x, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5432ffa",
   "metadata": {},
   "source": [
    "# Performance matrices / Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ec0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred):\n",
    "    def f(y_true, y_pred):\n",
    "        intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "        union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
    "        iou = K.mean((intersection + 1) / (union + 1), axis=0)\n",
    "        \n",
    "        return iou\n",
    "\n",
    "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
    "    return dice\n",
    "    \n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a43a32",
   "metadata": {},
   "source": [
    "# Federated Preperation Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1507354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data):\n",
    "    #get the bs\n",
    "    bs = 35\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data).numpy()*bs\n",
    "    return local_count/3626\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71533d",
   "metadata": {},
   "source": [
    "# Training code for Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1a3be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 116s 501ms/step - loss: 0.0583 - f1: 0.9434 - iou: 0.9171 - dice_coef: 0.9420\n"
     ]
    }
   ],
   "source": [
    "local_model = Resunetpp() \n",
    "client_1 = DataGenerator2D(\"E:/NU/TUsimple/train_set/training/clients/client_1/\", img_size=256, batch_size=8, shuffle=True)\n",
    "opt = tf.keras.optimizers.Adam(1e-3)\n",
    "metrics = [f1, iou , dice_coef ]\n",
    "\n",
    "local_model.compile(loss=dice_coef_loss,\n",
    "                      optimizer=opt,\n",
    "                      metrics=metrics)\n",
    "#set local model weight to the weight of the global model\n",
    "local_model.load_weights(r\"E:\\NU\\Federated\\updated_global\\GlobalWeights.h5\")\n",
    "        \n",
    "#fit local model with client's data\n",
    "local_model.fit(client_1, epochs=1, verbose=1)\n",
    "        \n",
    "#scale the model weights and add to list\n",
    "scaling_factor = 1/3 #maker sure to edit this line with a scaled weights based \n",
    "                     #on number of data samples for each client (no. of img for client x /no. of img for all clients )\n",
    "scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "local_model.set_weights(scaled_weights)\n",
    "local_model.save_weights(r\"E:\\NU\\Federated\\save_model\\client_1_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a5047",
   "metadata": {},
   "source": [
    "# Send the saved weights to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ae586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socket created successfully.\n",
      "Connection Established.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\NU\\\\Federated\\\\save_model\\\\client_1_weights.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33312\\3386722654.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Write File in binary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"E:\\NU\\Federated\\save_model\\client_1_weights.h5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\NU\\\\Federated\\\\save_model\\\\client_1_weights.h5'"
     ]
    }
   ],
   "source": [
    "#here to send the Path\\to\\save\\model\\for\\sending\\in_h5_formate.h5 file to server using API\n",
    "# Initialize Socket Instance\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "print (\"Socket created successfully.\")\n",
    "\n",
    "# Defining port and host\n",
    "TCP_IP = 'localhost'\n",
    "TCP_PORT = 9001\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "\n",
    "# Connect socket to the host and port\n",
    "sock.connect((TCP_IP, TCP_PORT))\n",
    "print('Connection Established.')\n",
    "# Send a greeting to the server\n",
    "# sock.send('A message from the client'.encode())\n",
    "\n",
    "# Write File in binary\n",
    "file = open(r\"E:\\NU\\Federated\\save_model\\client_1_weights.h5\", 'rb')\n",
    "\n",
    "\n",
    "line = file.read(BUFFER_SIZE)\n",
    "# Keep sending data to the client\n",
    "while(line):\n",
    "    line = file.read(BUFFER_SIZE)\n",
    "\n",
    "    file = open(r\"E:\\NU\\Federated\\save_model\\client_1_weights.h5\", 'rb')\n",
    "    line = file.read(1024)\n",
    "    # Keep sending data to the client\n",
    "    while(line):\n",
    "        sock.send(line)\n",
    "        line = file.read(1024)\n",
    "    file.close()\n",
    "    print('File has been transferred successfully.')\n",
    "\n",
    "print('File has been received successfully.')\n",
    "\n",
    "file.close()\n",
    "sock.close()\n",
    "print('Connection Closed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d1860",
   "metadata": {},
   "source": [
    "# Get the updated global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0172484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#here to get the UpdatedGlobalWeights.h5 file from server using API\n",
    "#\n",
    "TCP_IP = 'localhost'\n",
    "TCP_PORT = 9005\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.connect((TCP_IP, TCP_PORT))\n",
    "with open(r'E:\\NU\\Federated\\save_model\\client_1_weights_updated.h5', 'wb') as f:\n",
    "    print('file opened')\n",
    "    while True:\n",
    "        #print('receiving data...')\n",
    "        data = s.recv(BUFFER_SIZE)\n",
    "        print('data=%s', (data))\n",
    "        if not data:\n",
    "            f.close()\n",
    "            print('file close()')\n",
    "            break\n",
    "        # write data to a file\n",
    "        f.write(data)\n",
    "\n",
    "print('Successfully get the file')\n",
    "s.close()\n",
    "print('connection closed')\n",
    "#\n",
    "#os.remove(r\"E:\\NU\\Federated\\save_model\\client_1_weights.h5\")\n",
    "#source = (r\"E:\\NU\\Federated\\send_recieve\\updated.h5\")\n",
    "#destination = (r\"E:\\NU\\Federated\\save_model\\client_1_weights.h5\") #the saved model in edge device\n",
    "#shutil.copy(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a035a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ead0eed14181618a8c4058dc68539503edf8fba89331e9ab1f1ea187ca61c7e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
